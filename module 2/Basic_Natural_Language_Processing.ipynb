{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Natural Language Processing\n",
        "\n",
        "## What is Natural Language ?\n",
        "- Language used for everyday communication by humans\n",
        "\n",
        "## What is Natural Language Processing ?\n",
        "- Any computation, manipulation of natural language;\n",
        "- Natural languages evolve:\n",
        "  - New words get added;\n",
        "  - Old words lose popularity;\n",
        "  - Meanings of words change;\n",
        "  - Language rules themselves may change.\n",
        "\n",
        "## NLP Tasks: A Broad Spectrum\n",
        "- Counting words, counting frequency of words;\n",
        "- Finding sentence boundaries;\n",
        "- Part of speech tagging;\n",
        "- Parsing the sequence structure;\n",
        "- Identifying semantic roles;\n",
        "- Identifying entities in a sentence;\n",
        "- Finding which pronoun refers to wich entity."
      ],
      "metadata": {
        "id": "b00DDDzl36it"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic NLP tasks with NLTK\n",
        "\n",
        "## An Introduction to NLTK\n",
        "- NLTK: Natural Language Toolkit;\n",
        "- Open source library in Python;\n",
        "- Has support for most NLP tasks."
      ],
      "metadata": {
        "id": "O5Ki-bwS5IiQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRC7km6i317i",
        "outputId": "b127dc35-8f2e-44a1-bf39-41650c334601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]   Package udhr is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "nltk.download('udhr')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.book import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNm-aKK27DjX",
        "outputId": "65bc5afe-8ddd-49bc-fd0c-37188bd3626c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Text: Moby Dick by Herman Melville 1851>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBpgfszA7Ma8",
        "outputId": "8c9f3b66-36c0-4297-dea3-3445c0c40606"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sent1: Call me Ishmael .\n",
            "sent2: The family of Dashwood had long been settled in Sussex .\n",
            "sent3: In the beginning God created the heaven and the earth .\n",
            "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
            "sent5: I have a problem with people PMing me to lol JOIN\n",
            "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
            "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
            "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
            "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqNdOnkR7UQZ",
        "outputId": "19555ef3-8997-46b4-c845-d493f20a4ee6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Call', 'me', 'Ishmael', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting vocabulary of words\n",
        "sent7, len(sent7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBg3OwU17bGT",
        "outputId": "b5fff9b6-f52d-44c1-c209-89412d4faaeb"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Pierre',\n",
              "  'Vinken',\n",
              "  ',',\n",
              "  '61',\n",
              "  'years',\n",
              "  'old',\n",
              "  ',',\n",
              "  'will',\n",
              "  'join',\n",
              "  'the',\n",
              "  'board',\n",
              "  'as',\n",
              "  'a',\n",
              "  'nonexecutive',\n",
              "  'director',\n",
              "  'Nov.',\n",
              "  '29',\n",
              "  '.'],\n",
              " 18)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total words count = {len(text7)}\\nUnique words count = {len(set(text7))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYXWlEF47gYr",
        "outputId": "583642cb-3232-4981-9279-68d20b3b0fe8"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words count = 100676\n",
            "Unique words count = 12408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seeing the first words\n",
        "list(set(text7))[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q15oQybb79rm",
        "outputId": "782f8bd6-1068-4d7e-8025-fd9af82891dd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sellers', 'Soldado', 'Walt', 'motor-home', 'Editorials']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seeing the frequency of words\n",
        "dist = FreqDist(text7)\n",
        "print(len(dist))\n",
        "dist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO38O2oa8NdW",
        "outputId": "3600cbe4-731c-4b45-b5f0-9e5c413441fe"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12408\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({',': 4885, 'the': 4045, '.': 3828, 'of': 2319, 'to': 2164, 'a': 1878, 'in': 1572, 'and': 1511, '*-1': 1123, '0': 1099, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab7 = dist.keys()\n",
        "list(vocab7)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfO1eMa08cMt",
        "outputId": "8b09d4f0-bc1c-406a-f8e4-51fb495187b5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Pierre', 'Vinken', ',', '61', 'years']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding how many times a word appear\n",
        "dist['four']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzaXvQXt9HEY",
        "outputId": "17ca84d6-eca0-46f4-e62c-a7d7395720cc"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save frequent words if they have at least 5 letters\n",
        "freqwords = [w for w in vocab7 if len(w) > 5 and dist[w]>100]\n",
        "sorted(freqwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN0D4xWR9NIE",
        "outputId": "ca26a9ab-4a3f-47bd-e4e2-93875da0ab3a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['because',\n",
              " 'billion',\n",
              " 'company',\n",
              " 'market',\n",
              " 'million',\n",
              " 'president',\n",
              " 'program',\n",
              " 'shares',\n",
              " 'trading']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization and Stemming\n",
        "- Different forms of the same words:\n",
        "  - Example: \"List listed lists listing listings\"\n",
        "- Stemming them:\n",
        "  ```python\n",
        "  porter = nltk.PorterStemmer()\n",
        "  [porter.stem(t) for t in words]\n",
        "\n",
        "  >>> ['list','list','list','list','list']\n",
        "  ```\n"
      ],
      "metadata": {
        "id": "kU9oGeqMBe5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        " - When you care about the meaning of words instead of just removing suffixes"
      ],
      "metadata": {
        "id": "TtAKgDJHDEkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
        "udhr[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNXfVbSiDFGV",
        "outputId": "c4856a3f-b141-4437-c7d7-eefb7c163044"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Universal',\n",
              " 'Declaration',\n",
              " 'of',\n",
              " 'Human',\n",
              " 'Rights',\n",
              " 'Preamble',\n",
              " 'Whereas',\n",
              " 'recognition',\n",
              " 'of',\n",
              " 'the',\n",
              " 'inherent',\n",
              " 'dignity',\n",
              " 'and',\n",
              " 'of',\n",
              " 'the',\n",
              " 'equal',\n",
              " 'and',\n",
              " 'inalienable',\n",
              " 'rights',\n",
              " 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "[porter.stem(t) for t in udhr[:20]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIXqZ3V6DpyH",
        "outputId": "9594b694-9024-4083-c6d8-d3df2dee1e34"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['univers',\n",
              " 'declar',\n",
              " 'of',\n",
              " 'human',\n",
              " 'right',\n",
              " 'preambl',\n",
              " 'wherea',\n",
              " 'recognit',\n",
              " 'of',\n",
              " 'the',\n",
              " 'inher',\n",
              " 'digniti',\n",
              " 'and',\n",
              " 'of',\n",
              " 'the',\n",
              " 'equal',\n",
              " 'and',\n",
              " 'inalien',\n",
              " 'right',\n",
              " 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WNlemma = nltk.WordNetLemmatizer()\n",
        "[WNlemma.lemmatize(t) for t in udhr[:20]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTzNYL3oDyzK",
        "outputId": "ec5cce4a-f886-434d-b4d9-fa31c75b36a8"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Universal',\n",
              " 'Declaration',\n",
              " 'of',\n",
              " 'Human',\n",
              " 'Rights',\n",
              " 'Preamble',\n",
              " 'Whereas',\n",
              " 'recognition',\n",
              " 'of',\n",
              " 'the',\n",
              " 'inherent',\n",
              " 'dignity',\n",
              " 'and',\n",
              " 'of',\n",
              " 'the',\n",
              " 'equal',\n",
              " 'and',\n",
              " 'inalienable',\n",
              " 'right',\n",
              " 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "- Recall splitting a sentence into words/token can sometimes not work with you just split by blank space:\n",
        "[\"Children\", \"shouldn't\", \"drink\", \"alcohol\"]\n",
        "  - \"Shouldn't\" is counting as a single word.\n",
        "\n",
        "- NLTK has an in-built tokenizer"
      ],
      "metadata": {
        "id": "MiP2Uq7gEIcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
        "nltk.word_tokenize(text11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i74xGs97Eqxn",
        "outputId": "4d65d162-01ac-4d7d-df89-2bc247540173"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Children',\n",
              " 'should',\n",
              " \"n't\",\n",
              " 'drink',\n",
              " 'a',\n",
              " 'sugary',\n",
              " 'drink',\n",
              " 'before',\n",
              " 'bed',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Splitting\n",
        "- How would you split sentences from a long text string ?\n",
        "- NLTK has an in-built sentence splitter."
      ],
      "metadata": {
        "id": "kZo4TdjNFrkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
        "sentences = nltk.sent_tokenize(text12)\n",
        "print(sentences)\n",
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msNjakp8F0Wo",
        "outputId": "e0807dd1-9de5-4fcd-ec04-c2835af2ec33"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is the first sentence.', 'A gallon of milk in the U.S. costs $2.99.', 'Is this the third sentence?', 'Yes, it is!']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced NLP tasks with NLTK\n",
        "\n",
        "## Part-of-speech(POS) Tagging\n",
        "- Nouns, verbs, adjectives, ..."
      ],
      "metadata": {
        "id": "chbHOv3BGqHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets_json', quiet=True)\n",
        "nltk.help.upenn_tagset('MD')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aTSWSC7HF7x",
        "outputId": "be65d829-0d73-4c99-af67-fa77a425d121"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitted = nltk.word_tokenize(text11)\n",
        "nltk.download('averaged_perceptron_tagger_eng',quiet=True)\n",
        "\n",
        "# Especially useful when collecting verbs or nous, for example\n",
        "nltk.pos_tag(splitted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOJT2wVcHYrP",
        "outputId": "abab1d60-6f2a-47ff-db42-a91d81dcedab"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Children', 'NNP'),\n",
              " ('should', 'MD'),\n",
              " (\"n't\", 'RB'),\n",
              " ('drink', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('sugary', 'JJ'),\n",
              " ('drink', 'NN'),\n",
              " ('before', 'IN'),\n",
              " ('bed', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ambiguity in POS Taggin\n",
        "- Ambiguity is common in English"
      ],
      "metadata": {
        "id": "Ip7zv5oCISL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text14 = nltk.word_tokenize(\"Visiting aunts can be a nuisance\")\n",
        "\n",
        "# Visiting can be a verb in gerund form or an adjective for aunts\n",
        "nltk.pos_tag(text14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpqCqgJWIG9t",
        "outputId": "4e199c8f-ccd5-4465-aff2-328cf8170083"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Visiting', 'VBG'),\n",
              " ('aunts', 'NNS'),\n",
              " ('can', 'MD'),\n",
              " ('be', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('nuisance', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing Sentence Structure\n",
        "- Making sense of sentences is easy if they follow a well-defined grammatical structure"
      ],
      "metadata": {
        "id": "Mp0R9gYAJehi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text15 = nltk.word_tokenize(\"Alice loves Bob\")\n",
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "VP -> V NP\n",
        "NP -> 'Alice' | 'Bob'\n",
        "V -> 'loves'\n",
        "\"\"\")\n",
        "parser = nltk.ChartParser(grammar)\n",
        "trees = parser.parse_all(text15)\n",
        "for tree in trees:\n",
        "  print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SaSfu1ZJyVb",
        "outputId": "75e28ab2-fcfe-4fa0-c3da-1ff9c141b2ba"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP Alice) (VP (V loves) (NP Bob)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ambiguity in Parsing\n",
        "- Ambiguity may exist even if sentences are grammatically correct"
      ],
      "metadata": {
        "id": "UJnp1ay8KsWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text16 = nltk.word_tokenize(\"I saw the man with a telescope\")\n",
        "\n",
        "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "PP -> P NP\n",
        "NP -> Det N | Det N PP | 'I'\n",
        "VP -> V NP | VP PP\n",
        "Det -> 'a' | 'the'\n",
        "N -> 'man' | 'telescope'\n",
        "V -> 'saw'\n",
        "P -> 'with'\n",
        "\"\"\")\n",
        "\n",
        "parser = nltk.ChartParser(grammar1)\n",
        "trees = parser.parse_all(text16)\n",
        "for tree in trees:\n",
        "  print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNwr_hgMKrQ_",
        "outputId": "1ece6541-ef71-4dc1-80e5-15a4539ea71e"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (VP (V saw) (NP (Det the) (N man)))\n",
            "    (PP (P with) (NP (Det a) (N telescope)))))\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (V saw)\n",
            "    (NP (Det the) (N man) (PP (P with) (NP (Det a) (N telescope))))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK and Parse Tree Collection"
      ],
      "metadata": {
        "id": "ffLOCny3MQ2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import treebank\n",
        "text17 = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
        "print(text17)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II2KPBOGMUks",
        "outputId": "9e3d8da6-e5e3-45b5-dcad-f5f82265bfa9"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP-SBJ\n",
            "    (NP (NNP Pierre) (NNP Vinken))\n",
            "    (, ,)\n",
            "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
            "    (, ,))\n",
            "  (VP\n",
            "    (MD will)\n",
            "    (VP\n",
            "      (VB join)\n",
            "      (NP (DT the) (NN board))\n",
            "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
            "      (NP-TMP (NNP Nov.) (CD 29))))\n",
            "  (. .))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application: Spell Checker\n",
        "\n",
        "## Spelling Correction\n",
        "- A common way to check for mis-spelt words and correct them is to find valid words that share similar spelling;\n",
        "- Requires a dictionary of valid words:\n",
        "  - NLTK to the rescue: `import words from nltk.corpus`.\n",
        "- Requires some way to measure spelling similarity:\n",
        "  - \"Edit distance\" between two strings.\n",
        "\n",
        "## Edit distance\n",
        "- Number of changes that need to be made to string A to get to string B;\n",
        "- One specific algorithm: Levenshtein distance\n",
        "  - Insertions;\n",
        "  - Deletions;\n",
        "  - Substitutions.\n",
        "\n",
        "# N-grams\n",
        "- Character sequences in a word of size n;\n",
        "- Can also be used for word sequences;\n",
        "- How can n-grams help in spell-checking?\n",
        "  - If two words have similar spelling, they share many n-grams.\n",
        "  - Example with 2-grams:\n",
        "    - Correct word: pierce => pi, ie, er, rc, ce\n",
        "    - Mis-spelt word: pierse=> pi, ie, er, rs, se\n",
        "\n",
        "# Jaccard similarity\n",
        "- Used to measure similarity of sets;\n",
        "- Jaccard index/coefficient of similarity of two sets A and B is interaction of A and B / union of A and B;\n",
        "- Jaccard('pierce','pierse') = 3/7"
      ],
      "metadata": {
        "id": "ZNgvphyZT7RK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2- Introduction to NLTK\n",
        "\n",
        "In part 1 of this assignment you will use nltk to explore the <a href='http://www.cs.cmu.edu/~ark/personas/'>CMU Movie Summary Corpus</a>. All data is released under a <a href='https://creativecommons.org/licenses/by-sa/3.0/us/legalcode'>Creative Commons Attribution-ShareAlike License</a>. Then in part 2 you will create a spelling recommender function that uses nltk to find words similar to the misspelling."
      ],
      "metadata": {
        "id": "2iaSel5yZmLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Analyzing Plots Summary Text"
      ],
      "metadata": {
        "id": "MLLIPrqSZwrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 7\n",
        "\n",
        "`text1` is in `nltk.Text` format that has been constructed using tokens output by `nltk.word_tokenize(plots_raw)`.\n",
        "\n",
        "Now, use `nltk.sent_tokenize` on the tokens in `text1` by joining them using whitespace to output a sentence-tokenized copy of `text1`. Report the average number of whitespace separated tokens per sentence in the sentence-tokenized copy of `text1`.\n",
        "\n",
        "*This function should return a float.*"
      ],
      "metadata": {
        "id": "bUpmQzlN6Awu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4bdTFnZsxH",
        "outputId": "d3df3ed0-3c08-4480-a8b5-13df994eb681"
      },
      "source": [
        "sentences = nltk.sent_tokenize(\" \".join(text1))\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(sentences,columns=['sentences'])\n",
        "\n",
        "# Calculate the number of whitespace-separated tokens for each sentence\n",
        "df['token_count'] = df['sentences'].apply(lambda x: len(x.split()))\n",
        "\n",
        "df['token_count'].mean()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(26.067652643149795)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 9\n",
        "\n",
        "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
        "\n",
        "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.**\n",
        "\n",
        "Refer to:\n",
        "- [NLTK Jaccard distance](https://www.nltk.org/api/nltk.metrics.distance.html?highlight=jaccard_distance#nltk.metrics.distance.jaccard_distance)\n",
        "- [NLTK ngrams](https://www.nltk.org/api/nltk.util.html?highlight=ngrams#nltk.util.ngrams)\n",
        "\n",
        "*This function should return a list of length three:\n",
        "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
      ],
      "metadata": {
        "id": "88_RFz3sFMpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import words\n",
        "from nltk.util import ngrams\n",
        "from nltk.metrics.distance import jaccard_distance\n",
        "\n",
        "# nltk.download('words') # Already downloaded in the notebook's setup\n",
        "correct_spellings = words.words()\n",
        "\n",
        "entries=['cormulent', 'incendenece', 'validrate']\n",
        "\n",
        "def spell_checker(entries_list, correct_words_list):\n",
        "    recommendations = []\n",
        "    for entry_word in entries_list:\n",
        "        # Generate trigrams for the entry word\n",
        "        entry_trigrams = set(ngrams(entry_word, 3))\n",
        "\n",
        "        min_dist = 1.0 # Jaccard distance ranges from 0 to 1\n",
        "        best_match = None\n",
        "\n",
        "        for correct_word in correct_words_list:\n",
        "            # Ensure the correct word is long enough to form trigrams\n",
        "            if len(correct_word) >= 3:\n",
        "                correct_trigrams = set(ngrams(correct_word, 3))\n",
        "\n",
        "                # Calculate Jaccard distance\n",
        "                # Handle cases where trigram sets might be empty (though filtered for correct_word length)\n",
        "                if not entry_trigrams and not correct_trigrams:\n",
        "                    current_dist = 0.0\n",
        "                elif not entry_trigrams or not correct_trigrams:\n",
        "                    current_dist = 1.0\n",
        "                else:\n",
        "                    current_dist = jaccard_distance(entry_trigrams, correct_trigrams)\n",
        "\n",
        "                if current_dist < min_dist:\n",
        "                    min_dist = current_dist\n",
        "                    best_match = correct_word\n",
        "\n",
        "        # Add the best match or the original word if no suitable match was found\n",
        "        recommendations.append(best_match if best_match is not None else entry_word)\n",
        "    return recommendations\n",
        "\n",
        "# Call the function with the provided entries and correct spellings\n",
        "spell_recommendations = spell_checker(entries, correct_spellings)\n",
        "print(spell_recommendations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq4jGEM_FJ2X",
        "outputId": "4b2dfb52-799a-4da9-9058-908306d53548"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['formule', 'ascendence', 'validate']\n"
          ]
        }
      ]
    }
  ]
}